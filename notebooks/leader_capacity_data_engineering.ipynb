{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Leader Capacity Dashboard - Data Engineering Notebook\n",
        "\n",
        "## üìã Table of Contents\n",
        "1. [Cell 1-3] Project Overview & Setup\n",
        "2. [Cell 4-10] Data Loading\n",
        "3. [Cell 11-14] Data Processing\n",
        "4. [Cell 15-16] Dashboard Structure\n",
        "5. [Cell 17-18] Data Quality\n",
        "6. [Cell 19-20] Export & Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Cell 1] Project Overview\n",
        "\n",
        "This notebook handles the data engineering pipeline for recreating a leader capacity dashboard. The dashboard will display current month plus three future months of:\n",
        "- Booked time as % of available working time\n",
        "- Vacation/leave data\n",
        "- Salesforce opportunity data with likelihood and dates\n",
        "\n",
        "### Data Sources\n",
        "All data files are located in the `../data/` directory:\n",
        "1. **10k Data for S3 (1).csv** - Time booking/allocation data\n",
        "2. **10k Users.csv** - User roles and demographics (filtered to leadership roles)\n",
        "3. **Namely Vacation and Leave Dataset.csv** - Employee vacation and leave records\n",
        "4. **Salesforce Opportunity Data.csv** - Sales opportunities with probability and schedule\n",
        "5. **Working Hours For US.csv** - US working hours and holidays\n",
        "6. **UAE Working Hours.csv** - UAE working hours and holidays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All required packages are installed!\n",
            "üìç Using virtual environment at: venv/\n",
            "üöÄ You can proceed to Cell 3 to import the libraries\n"
          ]
        }
      ],
      "source": [
        "# [Cell 2] Package Installation Check\n",
        "# ‚úÖ All packages have been installed in the virtual environment!\n",
        "\n",
        "# If you ever need to reinstall packages:\n",
        "# !pip install pandas numpy matplotlib seaborn openpyxl\n",
        "\n",
        "print(\"‚úÖ All required packages are installed!\")\n",
        "print(\"üìç Using virtual environment at: venv/\")\n",
        "print(\"üöÄ You can proceed to Cell 3 to import the libraries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# [Cell 3] Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import calendar\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For visualization (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Cell 4] Data Loading Section\n",
        "\n",
        "### üîÑ Load all data sources with error handling\n",
        "We'll load each CSV file and explore its structure to understand what data we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Selected roles to filter:\n",
            "  ‚úì Design\n",
            "  ‚úì Principal\n",
            "  ‚úì Program Management\n",
            "  ‚úì Strategy\n",
            "  ‚úì Studio\n",
            "  ‚úì Tech\n",
            "\n",
            "üìä Total roles selected: 6\n",
            "\n",
            "‚úÖ 10k Data loaded successfully: (173658, 19)\n",
            "   Columns: ['role', 'discipline', 'client', 'phase_name', 'incurred_hours', 'scheduled_hours', 'difference_from_past_scheduled_hours', 'future_scheduled_hours', 'total_hours', 'RequestTodayDate']...\n",
            "\n",
            "‚úÖ 10k Users loaded successfully: (1045, 36)\n",
            "   Columns: ['last_login_time', 'billrate', 'id', 'first_name', 'last_name', 'account_owner', 'archived', 'billability_target', 'billable', 'created_at', 'deleted', 'deleted_at', 'discipline', 'display_name', 'email', 'employee_number', 'guid', 'hire_date', 'invitation_pending', 'license_type', 'location', 'location_id', 'mobile_phone', 'office_phone', 'role', 'termination_date', 'type', 'updated_at', 'user_settings', 'user_type_id', 'thumbnail', 'has_login', 'login_type', 'archived_at', '_BATCH_ID_', '_BATCH_LAST_RUN_']\n",
            "\n",
            "üìä Available roles in the data:\n",
            "role\n",
            "Design                204\n",
            "Strategy              126\n",
            "Program Management     95\n",
            "Principal              72\n",
            "xRegular               67\n",
            "Admin                  46\n",
            "xDirector              38\n",
            "xSenior                37\n",
            "Facilities             34\n",
            "Leadfully              31\n",
            "xAssociate             28\n",
            "Talent                 28\n",
            "Studio                 27\n",
            "Finance                19\n",
            "Exec and Team          19\n",
            "Brand                  15\n",
            "Business Ops           15\n",
            "IT                     14\n",
            "Biz Dev                14\n",
            "xManaging Director     11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üîç Filtering by role column: 'role'\n",
            "‚úÖ Filtered users: 528 out of 1045 total users\n",
            "\n",
            "üìä Role distribution after filtering:\n",
            "role\n",
            "Design                204\n",
            "Strategy              126\n",
            "Program Management     95\n",
            "Principal              72\n",
            "Studio                 27\n",
            "Tech                    4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üîó Merging datasets...\n",
            "\n",
            "üìã ID columns check:\n",
            "  ‚úì Found 'user_id' in 10k data\n",
            "  ‚úì Found 'id' in users data\n",
            "\n",
            "‚úÖ Merge successful! Result: (164349, 55)\n",
            "   164349 records for 224 unique users\n",
            "\n",
            "================================================================================\n",
            "üìä COMPREHENSIVE SAMPLE OF MERGED DATA\n",
            "================================================================================\n",
            "\n",
            "üìã Dataset Overview:\n",
            "   ‚Ä¢ Total records: 164,349\n",
            "   ‚Ä¢ Total columns: 55\n",
            "   ‚Ä¢ Unique users: 224\n",
            "   ‚Ä¢ Memory usage: 355.6 MB\n",
            "\n",
            "üîç DEBUG - All Available Columns in Merged Data:\n",
            "   üìä From 10k Data (first 10): ['client', 'phase_name', 'incurred_hours', 'scheduled_hours', 'difference_from_past_scheduled_hours', 'future_scheduled_hours', 'total_hours', 'RequestTodayDate', 'RequestToDate', 'RequestFromDate']\n",
            "   üë• From Users Data (first 10): ['last_login_time', 'billrate', 'id', 'first_name', 'last_name', 'account_owner', 'archived', 'billability_target', 'billable', 'created_at']\n",
            "   üìã All columns: ['role_x', 'discipline_x', 'client', 'phase_name', 'incurred_hours', 'scheduled_hours', 'difference_from_past_scheduled_hours', 'future_scheduled_hours', 'total_hours', 'RequestTodayDate', 'RequestToDate', 'RequestFromDate', 'project_id', 'project_name', 'user_id', 'user_name', 'date', '_BATCH_ID__x', '_BATCH_LAST_RUN__x', 'last_login_time', 'billrate', 'id', 'first_name', 'last_name', 'account_owner', 'archived', 'billability_target', 'billable', 'created_at', 'deleted', 'deleted_at', 'discipline_y', 'display_name', 'email', 'employee_number', 'guid', 'hire_date', 'invitation_pending', 'license_type', 'location', 'location_id', 'mobile_phone', 'office_phone', 'role_y', 'termination_date', 'type', 'updated_at', 'user_settings', 'user_type_id', 'thumbnail', 'has_login', 'login_type', 'archived_at', '_BATCH_ID__y', '_BATCH_LAST_RUN__y']\n",
            "\n",
            "üìä Sample of Key Columns (First 10 rows):\n",
            " user_id      id first_name last_name  client                         phase_name  incurred_hours  scheduled_hours  total_hours\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN SHF2103: Phase 01: Oct 14 - Oct 31             9.0              9.0          9.0\n",
            " 1361729 1361729      Nancy    Hawley     NaN   SHF2103: Phase 3: Nov 1 - Dec 16             9.0              9.0          9.0\n",
            "\n",
            "üìä Data Types for Key Columns:\n",
            "   ‚Ä¢ user_id: int64\n",
            "   ‚Ä¢ id: int64\n",
            "   ‚Ä¢ first_name: object\n",
            "   ‚Ä¢ last_name: object\n",
            "   ‚Ä¢ client: float64\n",
            "   ‚Ä¢ phase_name: object\n",
            "   ‚Ä¢ incurred_hours: float64\n",
            "   ‚Ä¢ scheduled_hours: float64\n",
            "   ‚Ä¢ total_hours: float64\n",
            "\n",
            "‚è∞ Hours-Related Columns Summary:\n",
            "   ‚Ä¢ incurred_hours: 164,349 non-null values, Total: 861829.3899999998\n",
            "   ‚Ä¢ scheduled_hours: 164,349 non-null values, Total: 739316.7999999999\n",
            "   ‚Ä¢ difference_from_past_scheduled_hours: 164,349 non-null values, Total: 170263.62\n",
            "   ‚Ä¢ future_scheduled_hours: 164,349 non-null values, Total: 47750.4\n",
            "   ‚Ä¢ total_hours: 164,349 non-null values, Total: 909579.7899999999\n",
            "\n",
            "üë• Sample User Information (First 5 unique users):\n",
            "     id first_name last_name         location                     email hire_date\n",
            "1361729      Nancy    Hawley Abu Dhabi | Flex    nhawley@sypartners.com       NaN\n",
            "  84323      Keela    Potter          NY Perm    kpotter@sypartners.com       NaN\n",
            " 522901       Mary Choueiter          NY Perm mchoueiter@sypartners.com       NaN\n",
            " 750761       Kate    Rinker          NY Perm    Krinker@sypartners.com       NaN\n",
            " 302023      Takuo    Fukuda          SF Perm    tfukuda@sypartners.com       NaN\n",
            "\n",
            "================================================================================\n",
            "‚úÖ VERIFICATION - Confirming filtered data contains ONLY selected roles:\n",
            "================================================================================\n",
            "\n",
            "üìä Final role distribution in filtered 10k data:\n",
            "\n",
            "================================================================================\n",
            "‚úÖ MERGE COMPLETE - Data ready for further processing!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# [Cell 5] Load and Process 10k Data - FILTERED BY SELECTED ROLES ONLY\n",
        "\n",
        "# Define the roles we want to include\n",
        "selected_roles = [\n",
        "    'Design',\n",
        "    'Principal', \n",
        "    'Program Management',\n",
        "    'Strategy',\n",
        "    'Studio',\n",
        "    'Tech'\n",
        "]\n",
        "\n",
        "print(\"üìã Selected roles to filter:\")\n",
        "for role in selected_roles:\n",
        "    print(f\"  ‚úì {role}\")\n",
        "print(f\"\\nüìä Total roles selected: {len(selected_roles)}\")\n",
        "print()\n",
        "\n",
        "# Load 10k Data\n",
        "try:\n",
        "    df_10k_data = pd.read_csv('../data/10k Data for S3 (1).csv')\n",
        "    print(f\"‚úÖ 10k Data loaded successfully: {df_10k_data.shape}\")\n",
        "    print(f\"   Columns: {df_10k_data.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading 10k data: {e}\")\n",
        "    df_10k_data = None\n",
        "\n",
        "# Load 10k Users\n",
        "try:\n",
        "    df_10k_users = pd.read_csv('../data/10k Users.csv')  # Note: capital 'U' in Users\n",
        "    print(f\"\\n‚úÖ 10k Users loaded successfully: {df_10k_users.shape}\")\n",
        "    print(f\"   Columns: {df_10k_users.columns.tolist()}\")\n",
        "    \n",
        "    # Check what roles are in the data\n",
        "    if 'role' in df_10k_users.columns:\n",
        "        print(f\"\\nüìä Available roles in the data:\")\n",
        "        role_counts = df_10k_users['role'].value_counts()\n",
        "        print(role_counts.head(20))\n",
        "    elif 'Role' in df_10k_users.columns:\n",
        "        print(f\"\\nüìä Available roles in the data:\")\n",
        "        role_counts = df_10k_users['Role'].value_counts()\n",
        "        print(role_counts.head(20))\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  No 'role' column found. Available columns:\")\n",
        "        print(df_10k_users.columns.tolist())\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading 10k users: {e}\")\n",
        "    df_10k_users = None\n",
        "\n",
        "# Filter users by selected roles\n",
        "if df_10k_users is not None:\n",
        "    # Find the correct role column name (could be 'role', 'Role', 'job_title', etc.)\n",
        "    role_column = None\n",
        "    for col in ['role', 'Role', 'job_title', 'Job_Title', 'position', 'Position']:\n",
        "        if col in df_10k_users.columns:\n",
        "            role_column = col\n",
        "            break\n",
        "    \n",
        "    if role_column:\n",
        "        print(f\"\\nüîç Filtering by role column: '{role_column}'\")\n",
        "        \n",
        "        # Filter users by selected roles\n",
        "        df_filtered_users = df_10k_users[df_10k_users[role_column].isin(selected_roles)]\n",
        "        print(f\"‚úÖ Filtered users: {df_filtered_users.shape[0]} out of {df_10k_users.shape[0]} total users\")\n",
        "        \n",
        "        # Show role distribution after filtering\n",
        "        print(f\"\\nüìä Role distribution after filtering:\")\n",
        "        print(df_filtered_users[role_column].value_counts())\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Could not find role column. Please check column names.\")\n",
        "        df_filtered_users = df_10k_users\n",
        "else:\n",
        "    df_filtered_users = None\n",
        "\n",
        "# Merge 10k data with filtered users\n",
        "if df_10k_data is not None and df_filtered_users is not None:\n",
        "    print(\"\\nüîó Merging datasets...\")\n",
        "    \n",
        "    # Check for ID columns\n",
        "    print(f\"\\nüìã ID columns check:\")\n",
        "    if 'user_id' in df_10k_data.columns:\n",
        "        print(f\"  ‚úì Found 'user_id' in 10k data\")\n",
        "    if 'id' in df_filtered_users.columns:\n",
        "        print(f\"  ‚úì Found 'id' in users data\")\n",
        "    \n",
        "    # Perform the merge\n",
        "    try:\n",
        "        df_merged = pd.merge(\n",
        "            df_10k_data,\n",
        "            df_filtered_users,\n",
        "            left_on='user_id',\n",
        "            right_on='id',\n",
        "            how='inner'  # Only keep records that match\n",
        "        )\n",
        "        print(f\"\\n‚úÖ Merge successful! Result: {df_merged.shape}\")\n",
        "        print(f\"   {df_merged.shape[0]} records for {df_merged['id'].nunique()} unique users\")\n",
        "        \n",
        "        # Save to use in later cells\n",
        "        df_10k = df_merged\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during merge: {e}\")\n",
        "        print(\"Please check that the ID columns exist and match correctly\")\n",
        "        df_10k = None\n",
        "else:\n",
        "    df_10k = None\n",
        "\n",
        "# Display comprehensive sample of merged data\n",
        "if df_10k is not None:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üìä COMPREHENSIVE SAMPLE OF MERGED DATA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Show basic info about the merged dataset\n",
        "    print(f\"\\nüìã Dataset Overview:\")\n",
        "    print(f\"   ‚Ä¢ Total records: {df_10k.shape[0]:,}\")\n",
        "    print(f\"   ‚Ä¢ Total columns: {df_10k.shape[1]}\")\n",
        "    print(f\"   ‚Ä¢ Unique users: {df_10k['id'].nunique()}\")\n",
        "    print(f\"   ‚Ä¢ Memory usage: {df_10k.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "    \n",
        "    # DEBUG: Show all available columns to help understand the structure\n",
        "    print(f\"\\nüîç DEBUG - All Available Columns in Merged Data:\")\n",
        "    if df_10k_data is not None:\n",
        "        print(\"   üìä From 10k Data (first 10):\", [col for col in df_10k.columns if col in df_10k_data.columns][:10])\n",
        "    if df_filtered_users is not None:\n",
        "        print(\"   üë• From Users Data (first 10):\", [col for col in df_10k.columns if col in df_filtered_users.columns][:10])\n",
        "    print(\"   üìã All columns:\", df_10k.columns.tolist())\n",
        "    \n",
        "    # Show key columns from both datasets\n",
        "    print(f\"\\nüìä Sample of Key Columns (First 10 rows):\")\n",
        "    key_cols = ['user_id', 'id', 'first_name', 'last_name', role_column if role_column else 'role', \n",
        "                'discipline', 'client', 'phase_name', 'incurred_hours', 'scheduled_hours', 'total_hours']\n",
        "    available_key_cols = [col for col in key_cols if col in df_10k.columns]\n",
        "    \n",
        "    sample_data = df_10k[available_key_cols].head(10)\n",
        "    print(sample_data.to_string(index=False))\n",
        "    \n",
        "    # Show data types for key columns\n",
        "    print(f\"\\nüìä Data Types for Key Columns:\")\n",
        "    for col in available_key_cols:\n",
        "        print(f\"   ‚Ä¢ {col}: {df_10k[col].dtype}\")\n",
        "    \n",
        "    # Show hours-related columns specifically\n",
        "    hours_cols = [col for col in df_10k.columns if 'hours' in col.lower()]\n",
        "    if hours_cols:\n",
        "        print(f\"\\n‚è∞ Hours-Related Columns Summary:\")\n",
        "        for col in hours_cols:\n",
        "            non_null_count = df_10k[col].notna().sum()\n",
        "            total_hours = df_10k[col].sum() if df_10k[col].dtype in ['int64', 'float64'] else 'N/A'\n",
        "            print(f\"   ‚Ä¢ {col}: {non_null_count:,} non-null values, Total: {total_hours}\")\n",
        "    \n",
        "    # Show sample of merged user info\n",
        "    print(f\"\\nüë• Sample User Information (First 5 unique users):\")\n",
        "    # Build list of user columns that actually exist in the dataframe\n",
        "    desired_user_cols = ['id', 'first_name', 'last_name', role_column if role_column else 'role', \n",
        "                         'discipline', 'location', 'email', 'hire_date']\n",
        "    available_user_cols = [col for col in desired_user_cols if col in df_10k.columns]\n",
        "    \n",
        "    if available_user_cols:\n",
        "        user_sample = df_10k[available_user_cols].drop_duplicates().head(5)\n",
        "        print(user_sample.to_string(index=False))\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No user information columns found in merged data\")\n",
        "    \n",
        "    # VERIFICATION: Confirm that ONLY selected roles are in the final dataset\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ VERIFICATION - Confirming filtered data contains ONLY selected roles:\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Check what roles are actually in the merged data\n",
        "    if role_column and role_column in df_10k.columns:\n",
        "        actual_roles = df_10k[role_column].unique()\n",
        "        print(f\"\\nüìã Roles found in final dataset:\")\n",
        "        for role in sorted(actual_roles):\n",
        "            if role in selected_roles:\n",
        "                print(f\"  ‚úì {role} (expected)\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå {role} (UNEXPECTED - should not be here!)\")\n",
        "        \n",
        "        # Double-check: Are all roles in the data part of selected_roles?\n",
        "        unexpected_roles = [r for r in actual_roles if r not in selected_roles]\n",
        "        if unexpected_roles:\n",
        "            print(f\"\\n‚ö†Ô∏è  WARNING: Found {len(unexpected_roles)} unexpected roles in the data!\")\n",
        "            print(f\"These roles should NOT be in the filtered data: {unexpected_roles}\")\n",
        "        else:\n",
        "            print(f\"\\n‚úÖ SUCCESS: All roles in the final dataset are from the selected list!\")\n",
        "            print(f\"   - {len(actual_roles)} unique roles found\")\n",
        "            print(f\"   - {df_10k.shape[0]:,} total records\")\n",
        "            print(f\"   - {df_10k['id'].nunique()} unique users\")\n",
        "    \n",
        "    # Show final role distribution\n",
        "    print(f\"\\nüìä Final role distribution in filtered 10k data:\")\n",
        "    if role_column and role_column in df_10k.columns:\n",
        "        role_dist = df_10k[role_column].value_counts()\n",
        "        print(role_dist)\n",
        "        \n",
        "        # Show records per user by role\n",
        "        print(f\"\\nüìä Average records per user by role:\")\n",
        "        user_counts = df_10k.groupby(role_column)['id'].nunique()\n",
        "        record_counts = df_10k[role_column].value_counts()\n",
        "        for role in sorted(actual_roles):\n",
        "            if role in user_counts.index and role in record_counts.index:\n",
        "                avg_records = record_counts[role] / user_counts[role]\n",
        "                print(f\"   ‚Ä¢ {role}: {user_counts[role]} users, {record_counts[role]:,} records, {avg_records:.1f} avg records/user\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ MERGE COMPLETE - Data ready for further processing!\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Cell 5a] Load and Integrate Employee Org Units (Dept, Div, Loc)\n",
        "\n",
        "We'll load a new dataset `Employee Org Units (Dept, Div, Loc)` that contains authoritative org unit fields per employee. Using `email` as the unique identifier, we'll:\n",
        "- Normalize column names and expected fields\n",
        "- Drop current `location`, `department`, and `division` from users/main datasets\n",
        "- Replace them with `Org_Office_Location`, `Org_Department`, `Org_Division` from this dataset\n",
        "\n",
        "Expected file path: `../data/Employee Org Units (Dept, Div, Loc).csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [Cell 5b] Read and merge Org Units by email; replace dept/div/location\n",
        "\n",
        "import os\n",
        "\n",
        "org_units_path = '../data/Employee Org Units (Dept, Div, Loc).csv'\n",
        "\n",
        "try:\n",
        "    df_org_units_raw = pd.read_csv(org_units_path)\n",
        "    print(f\"‚úÖ Org Units loaded: {df_org_units_raw.shape}\")\n",
        "    print(\"Columns:\", df_org_units_raw.columns.tolist())\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Org Units from {org_units_path}: {e}\")\n",
        "    df_org_units_raw = None\n",
        "\n",
        "# Normalize column names and pick canonical fields\n",
        "if df_org_units_raw is not None:\n",
        "    df_org_units = df_org_units_raw.copy()\n",
        "    # Lowercase and strip columns for robust matching\n",
        "    df_org_units.columns = [c.strip().lower() for c in df_org_units.columns]\n",
        "\n",
        "    # Map potential variants to canonical names\n",
        "    col_map = {}\n",
        "    def resolve(col_variants, fallback=None):\n",
        "        for v in col_variants:\n",
        "            if v.lower() in df_org_units.columns:\n",
        "                return v.lower()\n",
        "        return fallback\n",
        "\n",
        "    email_col = resolve(['email', 'work email', 'employee email', 'e-mail'])\n",
        "    dept_col = resolve(['department', 'dept'])\n",
        "    div_col = resolve(['division', 'div'])\n",
        "    loc_col = resolve(['office location', 'location', 'office', 'current office location'])\n",
        "\n",
        "    required = [email_col, dept_col, div_col, loc_col]\n",
        "    if any(col is None for col in required):\n",
        "        print(\"‚ö†Ô∏è Missing required columns in Org Units. Found:\", {\n",
        "            'email': email_col, 'department': dept_col, 'division': div_col, 'location': loc_col\n",
        "        })\n",
        "    else:\n",
        "        # Keep only necessary columns and rename to canonical schema\n",
        "        df_org_units = df_org_units[[email_col, dept_col, div_col, loc_col]].rename(columns={\n",
        "            email_col: 'email',\n",
        "            dept_col: 'Org_Department',\n",
        "            div_col: 'Org_Division',\n",
        "            loc_col: 'Org_Office_Location',\n",
        "        })\n",
        "\n",
        "        # Clean emails and drop dupes keeping the latest if timestamp available\n",
        "        df_org_units['email'] = df_org_units['email'].astype(str).str.strip().str.lower()\n",
        "        df_org_units = df_org_units.dropna(subset=['email']).drop_duplicates(subset=['email'], keep='last')\n",
        "\n",
        "        print(f\"‚úÖ Org Units normalized: {df_org_units.shape}\")\n",
        "        print(\"Sample:\")\n",
        "        print(df_org_units.head(5).to_string(index=False))\n",
        "\n",
        "        # Merge into filtered users\n",
        "        if 'df_filtered_users' in globals() and df_filtered_users is not None:\n",
        "            users = df_filtered_users.copy()\n",
        "            if 'email' in users.columns:\n",
        "                users['email'] = users['email'].astype(str).str.strip().str.lower()\n",
        "                original_cols = users.columns.tolist()\n",
        "                # Drop existing org columns if present\n",
        "                for c in ['location', 'department', 'division']:\n",
        "                    if c in users.columns:\n",
        "                        users = users.drop(columns=[c])\n",
        "                users = users.merge(df_org_units, on='email', how='left')\n",
        "                print(f\"‚úÖ Users merged with Org Units: {users.shape}\")\n",
        "                missing = users['Org_Department'].isna().sum()\n",
        "                print(f\"   üîé Users without Org Units match: {missing}\")\n",
        "                df_filtered_users = users\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è df_filtered_users has no 'email' column; cannot merge Org Units.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è df_filtered_users not available; run Cell 5 first.\")\n",
        "\n",
        "        # Merge into main 10k merged dataset\n",
        "        if 'df_10k' in globals() and df_10k is not None:\n",
        "            main = df_10k.copy()\n",
        "            if 'email' in main.columns:\n",
        "                main['email'] = main['email'].astype(str).str.strip().str.lower()\n",
        "                for c in ['location', 'department', 'division']:\n",
        "                    if c in main.columns:\n",
        "                        main = main.drop(columns=[c])\n",
        "                main = main.merge(df_org_units, on='email', how='left')\n",
        "                print(f\"‚úÖ Main df_10k merged with Org Units: {main.shape}\")\n",
        "                missing_main = main['Org_Department'].isna().sum()\n",
        "                print(f\"   üîé df_10k rows without Org Units match: {missing_main}\")\n",
        "                df_10k = main\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è df_10k has no 'email' column; cannot merge Org Units.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è df_10k not available; run Cell 5 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç DEBUG: Checking available dataframes\n",
            "============================================================\n",
            "‚úÖ df_10k found (global): (164349, 55)\n",
            "‚úÖ df_filtered_users found (global): (528, 36)\n",
            "‚úÖ df_10k_data found: (173658, 19)\n",
            "‚úÖ df_10k_users found: (1045, 36)\n",
            "\n",
            "üìä Summary:\n",
            "   Available variables: 4\n",
            "   Missing variables: 0\n",
            "\n",
            "üí° Tip: Variables must be created in the same kernel session to be available.\n",
            "   If you restarted the kernel, you need to re-run all cells in order.\n"
          ]
        }
      ],
      "source": [
        "# [Cell 6] Debug - Check Available Variables\n",
        "print(\"üîç DEBUG: Checking available dataframes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check what variables are available in the current session\n",
        "available_vars = []\n",
        "missing_vars = []\n",
        "\n",
        "# Check for df_10k\n",
        "if 'df_10k' in globals():\n",
        "    print(f\"‚úÖ df_10k found (global): {df_10k.shape}\")\n",
        "    available_vars.append('df_10k')\n",
        "elif 'df_10k' in locals():\n",
        "    print(f\"‚úÖ df_10k found (local): {df_10k.shape}\")\n",
        "    available_vars.append('df_10k')\n",
        "else:\n",
        "    print(\"‚ùå df_10k NOT found\")\n",
        "    missing_vars.append('df_10k')\n",
        "\n",
        "# Check for df_filtered_users\n",
        "if 'df_filtered_users' in globals():\n",
        "    print(f\"‚úÖ df_filtered_users found (global): {df_filtered_users.shape}\")\n",
        "    available_vars.append('df_filtered_users')\n",
        "elif 'df_filtered_users' in locals():\n",
        "    print(f\"‚úÖ df_filtered_users found (local): {df_filtered_users.shape}\")\n",
        "    available_vars.append('df_filtered_users')\n",
        "else:\n",
        "    print(\"‚ùå df_filtered_users NOT found\")\n",
        "    missing_vars.append('df_filtered_users')\n",
        "\n",
        "# Check for df_10k_data\n",
        "if 'df_10k_data' in globals():\n",
        "    print(f\"‚úÖ df_10k_data found: {df_10k_data.shape}\")\n",
        "    available_vars.append('df_10k_data')\n",
        "elif 'df_10k_data' in locals():\n",
        "    print(f\"‚úÖ df_10k_data found: {df_10k_data.shape}\")\n",
        "    available_vars.append('df_10k_data')\n",
        "else:\n",
        "    print(\"‚ùå df_10k_data NOT found\")\n",
        "    missing_vars.append('df_10k_data')\n",
        "\n",
        "# Check for df_10k_users\n",
        "if 'df_10k_users' in globals():\n",
        "    print(f\"‚úÖ df_10k_users found: {df_10k_users.shape}\")\n",
        "    available_vars.append('df_10k_users')\n",
        "elif 'df_10k_users' in locals():\n",
        "    print(f\"‚úÖ df_10k_users found: {df_10k_users.shape}\")\n",
        "    available_vars.append('df_10k_users')\n",
        "else:\n",
        "    print(\"‚ùå df_10k_users NOT found\")\n",
        "    missing_vars.append('df_10k_users')\n",
        "\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   Available variables: {len(available_vars)}\")\n",
        "print(f\"   Missing variables: {len(missing_vars)}\")\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"\\n‚ö†Ô∏è  To fix missing variables, please run:\")\n",
        "    if 'df_10k' in missing_vars:\n",
        "        print(\"   ‚Ä¢ Cell 5 - Load and Process 10k Data\")\n",
        "    \n",
        "print(\"\\nüí° Tip: Variables must be created in the same kernel session to be available.\")\n",
        "print(\"   If you restarted the kernel, you need to re-run all cells in order.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä EXPORTING DATAFRAMES TO CSV REVIEW FOLDER\n",
            "============================================================\n",
            "‚úÖ Exported df_10k: 164,349 rows, 55 columns\n",
            "‚úÖ Exported df_filtered_users: 528 rows, 36 columns\n",
            "‚úÖ Exported df_vacation: 933 rows, 21 columns\n",
            "‚úÖ Exported df_vacation_monthly: 258 rows, 8 columns\n",
            "\n",
            "üìã EXPORT SUMMARY\n",
            "============================================================\n",
            "üìÅ Files saved to: /Users/psweeney/leader-capacity-dashboard/CSV review\n",
            "üïê Timestamp: 20250711_113120\n",
            "\n",
            "üìä Exported DataFrames:\n",
            "\n",
            "‚Ä¢ df_10k:\n",
            "  - Description: Merged 10k booking data with leadership users\n",
            "  - Shape: 164,349 rows √ó 55 columns\n",
            "  - File: df_10k_merged_leadership_20250711_113120.csv\n",
            "\n",
            "‚Ä¢ df_filtered_users:\n",
            "  - Description: Leadership users only (filtered by role)\n",
            "  - Shape: 528 rows √ó 36 columns\n",
            "  - File: df_filtered_users_leadership_20250711_113120.csv\n",
            "\n",
            "‚Ä¢ df_vacation:\n",
            "  - Description: Detailed vacation records for leadership\n",
            "  - Shape: 933 rows √ó 21 columns\n",
            "  - File: df_vacation_detailed_20250711_113120.csv\n",
            "\n",
            "‚Ä¢ df_vacation_monthly:\n",
            "  - Description: Monthly vacation summary for dashboard\n",
            "  - Shape: 258 rows √ó 8 columns\n",
            "  - File: df_vacation_monthly_summary_20250711_113120.csv\n",
            "\n",
            "üìÑ Export summary saved to: export_summary_20250711_113120.csv\n",
            "\n",
            "üìù README created in CSV review folder\n",
            "\n",
            "‚úÖ Export complete! Check the '../CSV review' folder for your files.\n"
          ]
        }
      ],
      "source": [
        "# [Cell 6a] Export DataFrames to CSV Review Folder\n",
        "\n",
        "print(\"üìä EXPORTING DATAFRAMES TO CSV REVIEW FOLDER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create CSV review folder if it doesn't exist\n",
        "csv_folder = '../CSV review'\n",
        "os.makedirs(csv_folder, exist_ok=True)\n",
        "\n",
        "# Generate timestamp for file naming\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Dictionary to track what we're exporting\n",
        "export_summary = []\n",
        "\n",
        "# Export df_10k if it exists\n",
        "if 'df_10k' in globals() and df_10k is not None:\n",
        "    filename = f'df_10k_merged_leadership_{timestamp}.csv'\n",
        "    filepath = os.path.join(csv_folder, filename)\n",
        "    df_10k.to_csv(filepath, index=False)\n",
        "    export_summary.append({\n",
        "        'DataFrame': 'df_10k',\n",
        "        'Description': 'Merged 10k booking data with leadership users',\n",
        "        'Shape': df_10k.shape,\n",
        "        'Filename': filename\n",
        "    })\n",
        "    print(f\"‚úÖ Exported df_10k: {df_10k.shape[0]:,} rows, {df_10k.shape[1]} columns\")\n",
        "else:\n",
        "    print(\"‚ùå df_10k not found - please run Cell 5 first\")\n",
        "\n",
        "# Export df_filtered_users if it exists\n",
        "if 'df_filtered_users' in globals() and df_filtered_users is not None:\n",
        "    filename = f'df_filtered_users_leadership_{timestamp}.csv'\n",
        "    filepath = os.path.join(csv_folder, filename)\n",
        "    df_filtered_users.to_csv(filepath, index=False)\n",
        "    export_summary.append({\n",
        "        'DataFrame': 'df_filtered_users',\n",
        "        'Description': 'Leadership users only (filtered by role)',\n",
        "        'Shape': df_filtered_users.shape,\n",
        "        'Filename': filename\n",
        "    })\n",
        "    print(f\"‚úÖ Exported df_filtered_users: {df_filtered_users.shape[0]:,} rows, {df_filtered_users.shape[1]} columns\")\n",
        "else:\n",
        "    print(\"‚ùå df_filtered_users not found\")\n",
        "\n",
        "# Export vacation data if it exists\n",
        "if 'df_vacation' in globals() and df_vacation is not None:\n",
        "    filename = f'df_vacation_detailed_{timestamp}.csv'\n",
        "    filepath = os.path.join(csv_folder, filename)\n",
        "    df_vacation.to_csv(filepath, index=False)\n",
        "    export_summary.append({\n",
        "        'DataFrame': 'df_vacation',\n",
        "        'Description': 'Detailed vacation records for leadership',\n",
        "        'Shape': df_vacation.shape,\n",
        "        'Filename': filename\n",
        "    })\n",
        "    print(f\"‚úÖ Exported df_vacation: {df_vacation.shape[0]:,} rows, {df_vacation.shape[1]} columns\")\n",
        "else:\n",
        "    print(\"‚ùå df_vacation not found - please run Cell 7 first\")\n",
        "\n",
        "# Export monthly vacation summary if it exists\n",
        "if 'df_vacation_monthly' in globals() and df_vacation_monthly is not None:\n",
        "    filename = f'df_vacation_monthly_summary_{timestamp}.csv'\n",
        "    filepath = os.path.join(csv_folder, filename)\n",
        "    df_vacation_monthly.to_csv(filepath, index=False)\n",
        "    export_summary.append({\n",
        "        'DataFrame': 'df_vacation_monthly',\n",
        "        'Description': 'Monthly vacation summary for dashboard',\n",
        "        'Shape': df_vacation_monthly.shape,\n",
        "        'Filename': filename\n",
        "    })\n",
        "    print(f\"‚úÖ Exported df_vacation_monthly: {df_vacation_monthly.shape[0]:,} rows, {df_vacation_monthly.shape[1]} columns\")\n",
        "else:\n",
        "    print(\"‚ùå df_vacation_monthly not found\")\n",
        "\n",
        "# Create a summary report\n",
        "if export_summary:\n",
        "    print(f\"\\nüìã EXPORT SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üìÅ Files saved to: {os.path.abspath(csv_folder)}\")\n",
        "    print(f\"üïê Timestamp: {timestamp}\")\n",
        "    print(f\"\\nüìä Exported DataFrames:\")\n",
        "    \n",
        "    for item in export_summary:\n",
        "        print(f\"\\n‚Ä¢ {item['DataFrame']}:\")\n",
        "        print(f\"  - Description: {item['Description']}\")\n",
        "        print(f\"  - Shape: {item['Shape'][0]:,} rows √ó {item['Shape'][1]} columns\")\n",
        "        print(f\"  - File: {item['Filename']}\")\n",
        "    \n",
        "    # Also create a summary CSV with metadata\n",
        "    summary_df = pd.DataFrame(export_summary)\n",
        "    summary_filename = f'export_summary_{timestamp}.csv'\n",
        "    summary_filepath = os.path.join(csv_folder, summary_filename)\n",
        "    summary_df.to_csv(summary_filepath, index=False)\n",
        "    print(f\"\\nüìÑ Export summary saved to: {summary_filename}\")\n",
        "    \n",
        "    # Create a README for the CSV review folder\n",
        "    readme_content = f\"\"\"# CSV Review Folder\n",
        "    \n",
        "## Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "This folder contains exported DataFrames from the Leader Capacity Dashboard notebook.\n",
        "\n",
        "## Files in this export:\n",
        "\n",
        "\"\"\"\n",
        "    for item in export_summary:\n",
        "        readme_content += f\"\"\"\n",
        "### {item['Filename']}\n",
        "- **DataFrame**: {item['DataFrame']}\n",
        "- **Description**: {item['Description']}\n",
        "- **Shape**: {item['Shape'][0]:,} rows √ó {item['Shape'][1]} columns\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    readme_path = os.path.join(csv_folder, 'README.md')\n",
        "    with open(readme_path, 'w') as f:\n",
        "        f.write(readme_content)\n",
        "    \n",
        "    print(f\"\\nüìù README created in CSV review folder\")\n",
        "    print(f\"\\n‚úÖ Export complete! Check the '{csv_folder}' folder for your files.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No DataFrames were found to export.\")\n",
        "    print(\"   Please run the previous cells to create the DataFrames first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèñÔ∏è VACATION AND LEAVE DATA PROCESSING (ADJUSTED FOR AVAILABLE DATA)\n",
            "============================================================\n",
            "‚úÖ Vacation data loaded successfully: (18190, 20)\n",
            "   üìä Total records: 18,190\n",
            "   üìã Total columns: 20\n",
            "\n",
            "üèñÔ∏è Available vacation/leave types:\n",
            "Type\n",
            "Work From Anywhere              2171\n",
            "Jury Duty                       2100\n",
            "Family Caregiver Leave          2059\n",
            "Sick                            1534\n",
            "UAE Vacation                    1378\n",
            "Vacation                        1371\n",
            "Bereavement                     1361\n",
            "Parental Leave (UAE)            1319\n",
            "UAE Study Leave                 1313\n",
            "Family Caregiver Leave (UAE)    1313\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "üîç STEP 1: FILTERING VACATION DATA FOR LEADERSHIP ROLES\n",
            "============================================================\n",
            "‚úÖ Found filtered leadership users: 528 users\n",
            "   üìã Leadership names to match: 528\n",
            "   üìã Leadership employee numbers: 458\n",
            "\n",
            "‚úÖ Filtered vacation data: 8881 records\n",
            "   üìä From 459 unique employees\n",
            "\n",
            "üë• Leadership employees with vacation data:\n",
            "   ‚Ä¢ Aaron Fallon: 15 vacation records\n",
            "   ‚Ä¢ Abby  Brewster: 3 vacation records\n",
            "   ‚Ä¢ Abby  Ciucias: 2 vacation records\n",
            "   ‚Ä¢ Abby Brewster: 15 vacation records\n",
            "   ‚Ä¢ Abby Ciucias: 15 vacation records\n",
            "   ‚Ä¢ Adam  Chandler: 2 vacation records\n",
            "   ‚Ä¢ Adam Brick: 15 vacation records\n",
            "   ‚Ä¢ Adam Chandler: 15 vacation records\n",
            "   ‚Ä¢ Adam Chilton: 15 vacation records\n",
            "   ‚Ä¢ Adam Estabrook: 15 vacation records\n",
            "   ... and 449 more employees\n",
            "\n",
            "============================================================\n",
            "üîÑ STEP 2: PROCESSING VACATION DATA FOR DASHBOARD\n",
            "============================================================\n",
            "‚úÖ Processed vacation data: 933 records with actual time off\n",
            "\n",
            "üìÖ Vacation data date range:\n",
            "   ‚Ä¢ Earliest: 1994-10-17\n",
            "   ‚Ä¢ Latest: 2025-10-20\n",
            "\n",
            "üìä Vacation by category:\n",
            "Vacation_Category\n",
            "Vacation          629\n",
            "Sick Leave        153\n",
            "Family Leave      144\n",
            "Other               5\n",
            "Parental Leave      2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "üìä STEP 3: CREATING VACATION SUMMARY FOR DASHBOARD\n",
            "============================================================\n",
            "üìÖ Adjusted dashboard date range (based on available data):\n",
            "   ‚Ä¢ From: 2025-07\n",
            "   ‚Ä¢ To: 2025-10\n",
            "\n",
            "‚úÖ Vacation summary created: 320 month-person records\n",
            "   üìä Covering 153 unique employees\n",
            "   üìÖ Across 4 months\n",
            "\n",
            "üìä Sample vacation summary:\n",
            "       Full_Name      Month Vacation_Category  Days_Used  Days_Scheduled\n",
            "     Ellyn Heald 2025-07-01          Vacation        7.0             0.0\n",
            "      Mark Wanek 2025-08-01          Vacation       14.0             0.0\n",
            "      Mark Wanek 2025-09-01          Vacation        3.0             0.0\n",
            "   Sarah Lowndes 2025-08-01          Vacation        3.0             0.0\n",
            "   Chad Rochkind 2025-10-01          Vacation        5.0             0.0\n",
            "Steven  Richards 2025-10-01        Sick Leave        3.0             0.0\n",
            "Steven  Richards 2025-08-01      Family Leave        3.0             0.0\n",
            " Salma Aldarmaki 2025-10-01          Vacation        3.0             0.0\n",
            "  Sydni Francois 2025-09-01          Vacation        8.0             0.0\n",
            "  Sydni Francois 2025-08-01      Family Leave        3.0             0.0\n",
            "\n",
            "üìä Monthly vacation totals:\n",
            "            Total_Days_Used  Total_Days_Scheduled  Unique_Employees\n",
            "Month                                                              \n",
            "2025-07-01            410.0                   0.0                68\n",
            "2025-08-01            473.0                   0.0                63\n",
            "2025-09-01            409.0                   0.0                67\n",
            "2025-10-01            344.0                   0.0                61\n",
            "\n",
            "============================================================\n",
            "üîó STEP 4: PREPARING FOR INTEGRATION WITH MAIN DATASET\n",
            "============================================================\n",
            "‚úÖ Found df_10k dataset for integration\n",
            "   üìä df_10k shape: (164349, 55)\n",
            "\n",
            "üìä Monthly vacation data ready for merge: 259 records\n",
            "   üë• For 153 unique employees\n",
            "\n",
            "‚úÖ Vacation data processing complete!\n",
            "   üìä df_vacation: 933 detailed records\n",
            "   üìä df_vacation_monthly: 259 monthly summaries\n",
            "\n",
            "============================================================\n",
            "‚úÖ VACATION DATA PROCESSING COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# [Cell 7] Load, Process, and Integrate Vacation Data (Fixed Date Range)\n",
        "\n",
        "print(\"üèñÔ∏è VACATION AND LEAVE DATA PROCESSING (ADJUSTED FOR AVAILABLE DATA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load vacation data\n",
        "try:\n",
        "    df_vacation_raw = pd.read_csv('../data/Namely Vacation and Leave Dataset.csv')\n",
        "    print(f\"‚úÖ Vacation data loaded successfully: {df_vacation_raw.shape}\")\n",
        "    print(f\"   üìä Total records: {df_vacation_raw.shape[0]:,}\")\n",
        "    print(f\"   üìã Total columns: {df_vacation_raw.shape[1]}\")\n",
        "    \n",
        "    # Show vacation types available\n",
        "    print(f\"\\nüèñÔ∏è Available vacation/leave types:\")\n",
        "    vacation_types = df_vacation_raw['Type'].value_counts()\n",
        "    print(vacation_types.head(10))\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading vacation data: {e}\")\n",
        "    df_vacation_raw = None\n",
        "\n",
        "if df_vacation_raw is not None:\n",
        "    \n",
        "    # STEP 1: Filter vacation data for leadership roles only\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üîç STEP 1: FILTERING VACATION DATA FOR LEADERSHIP ROLES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # We need to match vacation data with our leadership users\n",
        "    # Check if we have the filtered users from Cell 5\n",
        "    if 'df_filtered_users' in globals() and df_filtered_users is not None:\n",
        "        print(f\"‚úÖ Found filtered leadership users: {df_filtered_users.shape[0]} users\")\n",
        "        \n",
        "        # Create matching datasets\n",
        "        leadership_names = []\n",
        "        leadership_employee_numbers = []\n",
        "        \n",
        "        # Collect names and employee numbers from leadership users\n",
        "        for _, user in df_filtered_users.iterrows():\n",
        "            # Add full names\n",
        "            full_name = f\"{user['first_name']} {user['last_name']}\"\n",
        "            leadership_names.append(full_name)\n",
        "            \n",
        "            # Add employee numbers if available\n",
        "            if 'employee_number' in user and pd.notna(user['employee_number']):\n",
        "                leadership_employee_numbers.append(user['employee_number'])\n",
        "        \n",
        "        print(f\"   üìã Leadership names to match: {len(leadership_names)}\")\n",
        "        print(f\"   üìã Leadership employee numbers: {len(leadership_employee_numbers)}\")\n",
        "        \n",
        "        # Filter vacation data by matching names and employee numbers\n",
        "        name_matches = df_vacation_raw['Full Name'].isin(leadership_names)\n",
        "        emp_num_matches = df_vacation_raw['Employee Number'].isin(leadership_employee_numbers)\n",
        "        vacation_filtered = df_vacation_raw[name_matches | emp_num_matches].copy()\n",
        "        \n",
        "        print(f\"\\n‚úÖ Filtered vacation data: {vacation_filtered.shape[0]} records\")\n",
        "        print(f\"   üìä From {vacation_filtered['Full Name'].nunique()} unique employees\")\n",
        "        \n",
        "        # Show which leadership people have vacation data\n",
        "        matched_names = vacation_filtered['Full Name'].unique()\n",
        "        print(f\"\\nüë• Leadership employees with vacation data:\")\n",
        "        for name in sorted(matched_names)[:10]:  # Show first 10\n",
        "            count = vacation_filtered[vacation_filtered['Full Name'] == name].shape[0]\n",
        "            print(f\"   ‚Ä¢ {name}: {count} vacation records\")\n",
        "        if len(matched_names) > 10:\n",
        "            print(f\"   ... and {len(matched_names) - 10} more employees\")\n",
        "            \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No filtered users found from Cell 5. Using all vacation data.\")\n",
        "        print(\"   üí° Please run Cell 5 first to filter for leadership roles only.\")\n",
        "        vacation_filtered = df_vacation_raw.copy()\n",
        "    \n",
        "    # STEP 2: Process vacation data for dashboard use\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üîÑ STEP 2: PROCESSING VACATION DATA FOR DASHBOARD\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Clean and process the vacation data\n",
        "    vacation_processed = vacation_filtered.copy()\n",
        "    \n",
        "    # Convert date columns\n",
        "    date_columns = ['Start date', 'Departure date']\n",
        "    for col in date_columns:\n",
        "        if col in vacation_processed.columns:\n",
        "            vacation_processed[col] = pd.to_datetime(vacation_processed[col], errors='coerce')\n",
        "    \n",
        "    # Focus on actual vacation/leave (not just allocations)\n",
        "    vacation_actual = vacation_processed[\n",
        "        (vacation_processed['Used'] > 0) | (vacation_processed['Scheduled'] > 0)\n",
        "    ].copy()\n",
        "    \n",
        "    print(f\"‚úÖ Processed vacation data: {vacation_actual.shape[0]} records with actual time off\")\n",
        "    \n",
        "    # Check date range of vacation data\n",
        "    if not vacation_actual.empty and vacation_actual['Start date'].notna().any():\n",
        "        date_min = vacation_actual['Start date'].min()\n",
        "        date_max = vacation_actual['Start date'].max()\n",
        "        print(f\"\\nüìÖ Vacation data date range:\")\n",
        "        print(f\"   ‚Ä¢ Earliest: {date_min.strftime('%Y-%m-%d')}\")\n",
        "        print(f\"   ‚Ä¢ Latest: {date_max.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Categorize vacation types for dashboard\n",
        "    vacation_categories = {\n",
        "        'Vacation': ['Vacation', 'UAE Vacation', 'Work From Anywhere'],\n",
        "        'Sick Leave': ['Sick', 'UAE Sick Time'],\n",
        "        'Parental Leave': ['Parental Leave (UAE)', 'Prenatal Leave'],\n",
        "        'Family Leave': ['Family Caregiver Leave', 'Family Caregiver Leave (UAE)', 'Bereavement'],\n",
        "        'Other': ['Jury Duty', 'UAE Study Leave']\n",
        "    }\n",
        "    \n",
        "    def categorize_vacation_type(vacation_type):\n",
        "        for category, types in vacation_categories.items():\n",
        "            if vacation_type in types:\n",
        "                return category\n",
        "        return 'Other'\n",
        "    \n",
        "    vacation_actual = vacation_actual.copy()\n",
        "    vacation_actual['Vacation_Category'] = vacation_actual['Type'].apply(categorize_vacation_type)\n",
        "    \n",
        "    print(f\"\\nüìä Vacation by category:\")\n",
        "    print(vacation_actual['Vacation_Category'].value_counts())\n",
        "    \n",
        "    # STEP 3: Create vacation summary for dashboard integration\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä STEP 3: CREATING VACATION SUMMARY FOR DASHBOARD\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # ADJUSTED: Use the last 4 months of available data instead of future months\n",
        "    if not vacation_actual.empty and vacation_actual['Start date'].notna().any():\n",
        "        latest_date = vacation_actual['Start date'].max()\n",
        "        # Go back 4 months from the latest date\n",
        "        start_date = latest_date - pd.DateOffset(months=3)\n",
        "        start_date = start_date.replace(day=1)  # Start of month\n",
        "        \n",
        "        date_range = pd.date_range(\n",
        "            start=start_date,\n",
        "            periods=4,\n",
        "            freq='MS'  # Month start\n",
        "        )\n",
        "        \n",
        "        print(f\"üìÖ Adjusted dashboard date range (based on available data):\")\n",
        "        print(f\"   ‚Ä¢ From: {date_range[0].strftime('%Y-%m')}\")\n",
        "        print(f\"   ‚Ä¢ To: {date_range[-1].strftime('%Y-%m')}\")\n",
        "    else:\n",
        "        # Fallback to default range\n",
        "        current_date = pd.Timestamp.now()\n",
        "        date_range = pd.date_range(\n",
        "            start=current_date.replace(day=1),\n",
        "            periods=4,\n",
        "            freq='MS'\n",
        "        )\n",
        "        print(f\"üìÖ Using default date range: {date_range[0].strftime('%Y-%m')} to {date_range[-1].strftime('%Y-%m')}\")\n",
        "    \n",
        "    # Create vacation summary by person and month\n",
        "    vacation_summary = []\n",
        "    \n",
        "    for _, row in vacation_actual.iterrows():\n",
        "        start_date = row['Start date']\n",
        "        departure_date = row['Departure date']\n",
        "        \n",
        "        # Skip if no valid dates\n",
        "        if pd.isna(start_date):\n",
        "            continue\n",
        "            \n",
        "        # Use departure date if available, otherwise assume single day\n",
        "        end_date = departure_date if pd.notna(departure_date) else start_date\n",
        "        \n",
        "        # Check if vacation overlaps with our dashboard period\n",
        "        for month_start in date_range:\n",
        "            month_end = month_start + pd.offsets.MonthEnd(0)\n",
        "            \n",
        "            # Check if vacation overlaps with this month\n",
        "            if start_date <= month_end and end_date >= month_start:\n",
        "                vacation_summary.append({\n",
        "                    'Full_Name': row['Full Name'],\n",
        "                    'First_Name': row['First Name'],\n",
        "                    'Last_Name': row['Last Name'],\n",
        "                    'Employee_Number': row['Employee Number'],\n",
        "                    'Month': month_start,\n",
        "                    'Vacation_Type': row['Type'],\n",
        "                    'Vacation_Category': row['Vacation_Category'],\n",
        "                    'Days_Used': row['Used'],\n",
        "                    'Days_Scheduled': row['Scheduled'],\n",
        "                    'Start_Date': start_date,\n",
        "                    'End_Date': end_date,\n",
        "                    'Job_Title': row['Job Title'],\n",
        "                    'Office_Location': row['Office Location']\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df_vacation_summary = pd.DataFrame(vacation_summary)\n",
        "    \n",
        "    if not df_vacation_summary.empty:\n",
        "        print(f\"\\n‚úÖ Vacation summary created: {df_vacation_summary.shape[0]} month-person records\")\n",
        "        print(f\"   üìä Covering {df_vacation_summary['Full_Name'].nunique()} unique employees\")\n",
        "        print(f\"   üìÖ Across {df_vacation_summary['Month'].nunique()} months\")\n",
        "        \n",
        "        # Show sample of vacation summary\n",
        "        print(f\"\\nüìä Sample vacation summary:\")\n",
        "        sample_cols = ['Full_Name', 'Month', 'Vacation_Category', 'Days_Used', 'Days_Scheduled']\n",
        "        print(df_vacation_summary[sample_cols].head(10).to_string(index=False))\n",
        "        \n",
        "        # Show monthly vacation totals\n",
        "        print(f\"\\nüìä Monthly vacation totals:\")\n",
        "        monthly_totals = df_vacation_summary.groupby('Month').agg({\n",
        "            'Days_Used': 'sum',\n",
        "            'Days_Scheduled': 'sum',\n",
        "            'Full_Name': 'nunique'\n",
        "        }).round(1)\n",
        "        monthly_totals.columns = ['Total_Days_Used', 'Total_Days_Scheduled', 'Unique_Employees']\n",
        "        print(monthly_totals)\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No vacation data found for the selected period\")\n",
        "        df_vacation_summary = pd.DataFrame()\n",
        "    \n",
        "    # STEP 4: Prepare for integration with main dataset\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üîó STEP 4: PREPARING FOR INTEGRATION WITH MAIN DATASET\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Create a version that can be merged with the main 10k dataset\n",
        "    if not df_vacation_summary.empty:\n",
        "        # Check if df_10k exists\n",
        "        if 'df_10k' in globals() and df_10k is not None:\n",
        "            print(\"‚úÖ Found df_10k dataset for integration\")\n",
        "            print(f\"   üìä df_10k shape: {df_10k.shape}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  df_10k not found - please run Cell 5 first\")\n",
        "            print(\"   üí° The vacation data is still processed and ready for later use\")\n",
        "        \n",
        "        # Group by person and month to avoid duplicates\n",
        "        vacation_monthly = df_vacation_summary.groupby(['Full_Name', 'Month']).agg({\n",
        "            'Days_Used': 'sum',\n",
        "            'Days_Scheduled': 'sum',\n",
        "            'Vacation_Category': lambda x: ', '.join(x.unique()),\n",
        "            'First_Name': 'first',\n",
        "            'Last_Name': 'first',\n",
        "            'Employee_Number': 'first'\n",
        "        }).reset_index()\n",
        "        \n",
        "        print(f\"\\nüìä Monthly vacation data ready for merge: {vacation_monthly.shape[0]} records\")\n",
        "        print(f\"   üë• For {vacation_monthly['Full_Name'].nunique()} unique employees\")\n",
        "        \n",
        "        # Store for use in later cells\n",
        "        df_vacation = vacation_actual  # Full detailed data\n",
        "        df_vacation_monthly = vacation_monthly  # Monthly summary for dashboard\n",
        "        \n",
        "        print(f\"\\n‚úÖ Vacation data processing complete!\")\n",
        "        print(f\"   üìä df_vacation: {df_vacation.shape[0]} detailed records\")\n",
        "        print(f\"   üìä df_vacation_monthly: {df_vacation_monthly.shape[0]} monthly summaries\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No vacation summary to prepare\")\n",
        "        df_vacation = vacation_actual if 'vacation_actual' in locals() else pd.DataFrame()\n",
        "        df_vacation_monthly = pd.DataFrame()\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ VACATION DATA PROCESSING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Cannot process vacation data - loading failed\")\n",
        "    df_vacation = None\n",
        "    df_vacation_monthly = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèñÔ∏è VACATION AND LEAVE DATA PROCESSING\n",
            "============================================================\n",
            "‚úÖ Vacation data loaded successfully: (18190, 20)\n",
            "   üìä Total records: 18,190\n",
            "   üìã Total columns: 20\n",
            "\n",
            "üèñÔ∏è Available vacation/leave types:\n",
            "Type\n",
            "Work From Anywhere              2171\n",
            "Jury Duty                       2100\n",
            "Family Caregiver Leave          2059\n",
            "Sick                            1534\n",
            "UAE Vacation                    1378\n",
            "Vacation                        1371\n",
            "Bereavement                     1361\n",
            "Parental Leave (UAE)            1319\n",
            "UAE Study Leave                 1313\n",
            "Family Caregiver Leave (UAE)    1313\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "üîç STEP 1: FILTERING VACATION DATA FOR LEADERSHIP ROLES\n",
            "============================================================\n",
            "‚úÖ Found filtered leadership users: 528 users\n",
            "   üìã Leadership names to match: 528\n",
            "   üìã Leadership employee numbers: 458\n",
            "\n",
            "‚úÖ Filtered vacation data: 8881 records\n",
            "   üìä From 459 unique employees\n",
            "\n",
            "üë• Leadership employees with vacation data:\n",
            "   ‚Ä¢ Aaron Fallon: 15 vacation records\n",
            "   ‚Ä¢ Abby  Brewster: 3 vacation records\n",
            "   ‚Ä¢ Abby  Ciucias: 2 vacation records\n",
            "   ‚Ä¢ Abby Brewster: 15 vacation records\n",
            "   ‚Ä¢ Abby Ciucias: 15 vacation records\n",
            "   ‚Ä¢ Adam  Chandler: 2 vacation records\n",
            "   ‚Ä¢ Adam Brick: 15 vacation records\n",
            "   ‚Ä¢ Adam Chandler: 15 vacation records\n",
            "   ‚Ä¢ Adam Chilton: 15 vacation records\n",
            "   ‚Ä¢ Adam Estabrook: 15 vacation records\n",
            "   ... and 449 more employees\n",
            "\n",
            "============================================================\n",
            "üîÑ STEP 2: PROCESSING VACATION DATA FOR DASHBOARD\n",
            "============================================================\n",
            "‚úÖ Processed vacation data: 933 records with actual time off\n",
            "\n",
            "üìä Vacation by category:\n",
            "Vacation_Category\n",
            "Vacation          629\n",
            "Sick Leave        153\n",
            "Family Leave      144\n",
            "Other               5\n",
            "Parental Leave      2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "üìä STEP 3: CREATING VACATION SUMMARY FOR DASHBOARD\n",
            "============================================================\n",
            "üìÖ Dashboard date range: 2025-07 to 2025-10\n",
            "‚úÖ Vacation summary created: 319 month-person records\n",
            "   üìä Covering 153 unique employees\n",
            "   üìÖ Across 4 months\n",
            "\n",
            "üìä Sample vacation summary:\n",
            "       Full_Name                      Month Vacation_Category  Days_Used  Days_Scheduled\n",
            "     Ellyn Heald 2025-07-01 11:24:08.375646          Vacation        7.0             0.0\n",
            "      Mark Wanek 2025-08-01 11:24:08.375646          Vacation       14.0             0.0\n",
            "      Mark Wanek 2025-09-01 11:24:08.375646          Vacation        3.0             0.0\n",
            "   Sarah Lowndes 2025-08-01 11:24:08.375646          Vacation        3.0             0.0\n",
            "   Chad Rochkind 2025-10-01 11:24:08.375646          Vacation        5.0             0.0\n",
            "Steven  Richards 2025-10-01 11:24:08.375646        Sick Leave        3.0             0.0\n",
            "Steven  Richards 2025-08-01 11:24:08.375646      Family Leave        3.0             0.0\n",
            " Salma Aldarmaki 2025-10-01 11:24:08.375646          Vacation        3.0             0.0\n",
            "  Sydni Francois 2025-09-01 11:24:08.375646          Vacation        8.0             0.0\n",
            "  Sydni Francois 2025-08-01 11:24:08.375646      Family Leave        3.0             0.0\n",
            "\n",
            "üìä Monthly vacation totals:\n",
            "                            Total_Days_Used  Total_Days_Scheduled  \\\n",
            "Month                                                               \n",
            "2025-07-01 11:24:08.375646            410.0                   0.0   \n",
            "2025-08-01 11:24:08.375646            472.0                   0.0   \n",
            "2025-09-01 11:24:08.375646            409.0                   0.0   \n",
            "2025-10-01 11:24:08.375646            344.0                   0.0   \n",
            "\n",
            "                            Unique_Employees  \n",
            "Month                                         \n",
            "2025-07-01 11:24:08.375646                68  \n",
            "2025-08-01 11:24:08.375646                62  \n",
            "2025-09-01 11:24:08.375646                67  \n",
            "2025-10-01 11:24:08.375646                61  \n",
            "\n",
            "============================================================\n",
            "üîó STEP 4: PREPARING FOR INTEGRATION WITH MAIN DATASET\n",
            "============================================================\n",
            "‚úÖ Attempting to merge vacation data with main 10k dataset...\n",
            "   üìä Monthly vacation data ready for merge: 258 records\n",
            "   üë• For 153 unique employees\n",
            "\n",
            "‚úÖ Vacation data processing complete!\n",
            "   üìä df_vacation: 933 detailed records\n",
            "   üìä df_vacation_monthly: 258 monthly summaries\n",
            "\n",
            "============================================================\n",
            "‚úÖ VACATION DATA PROCESSING COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# [Cell 6] Load, Process, and Integrate Vacation and Leave Data\n",
        "\n",
        "print(\"üèñÔ∏è VACATION AND LEAVE DATA PROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load vacation data\n",
        "try:\n",
        "    df_vacation_raw = pd.read_csv('../data/Namely Vacation and Leave Dataset.csv')\n",
        "    print(f\"‚úÖ Vacation data loaded successfully: {df_vacation_raw.shape}\")\n",
        "    print(f\"   üìä Total records: {df_vacation_raw.shape[0]:,}\")\n",
        "    print(f\"   üìã Total columns: {df_vacation_raw.shape[1]}\")\n",
        "    \n",
        "    # Show vacation types available\n",
        "    print(f\"\\nüèñÔ∏è Available vacation/leave types:\")\n",
        "    vacation_types = df_vacation_raw['Type'].value_counts()\n",
        "    print(vacation_types.head(10))\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading vacation data: {e}\")\n",
        "    df_vacation_raw = None\n",
        "\n",
        "if df_vacation_raw is not None:\n",
        "    \n",
        "    # STEP 1: Filter vacation data for leadership roles only\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üîç STEP 1: FILTERING VACATION DATA FOR LEADERSHIP ROLES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # We need to match vacation data with our leadership users\n",
        "    # Check if we have the filtered users from Cell 5\n",
        "    if 'df_filtered_users' in locals() and df_filtered_users is not None:\n",
        "        print(f\"‚úÖ Found filtered leadership users: {df_filtered_users.shape[0]} users\")\n",
        "        \n",
        "        # Create matching datasets - try multiple approaches\n",
        "        leadership_names = []\n",
        "        leadership_employee_numbers = []\n",
        "        \n",
        "        # Collect names and employee numbers from leadership users\n",
        "        for _, user in df_filtered_users.iterrows():\n",
        "            # Add full names\n",
        "            full_name = f\"{user['first_name']} {user['last_name']}\"\n",
        "            leadership_names.append(full_name)\n",
        "            \n",
        "            # Add employee numbers if available\n",
        "            if 'employee_number' in user and pd.notna(user['employee_number']):\n",
        "                leadership_employee_numbers.append(user['employee_number'])\n",
        "        \n",
        "        print(f\"   üìã Leadership names to match: {len(leadership_names)}\")\n",
        "        print(f\"   üìã Leadership employee numbers: {len(leadership_employee_numbers)}\")\n",
        "        \n",
        "        # Filter vacation data by matching names and employee numbers\n",
        "        name_matches = df_vacation_raw['Full Name'].isin(leadership_names)\n",
        "        emp_num_matches = df_vacation_raw['Employee Number'].isin(leadership_employee_numbers)\n",
        "        vacation_filtered = df_vacation_raw[name_matches | emp_num_matches].copy()\n",
        "        \n",
        "        print(f\"\\n‚úÖ Filtered vacation data: {vacation_filtered.shape[0]} records\")\n",
        "        print(f\"   üìä From {vacation_filtered['Full Name'].nunique()} unique employees\")\n",
        "        \n",
        "        # Show which leadership people have vacation data\n",
        "        matched_names = vacation_filtered['Full Name'].unique()\n",
        "        print(f\"\\nüë• Leadership employees with vacation data:\")\n",
        "        for name in sorted(matched_names)[:10]:  # Show first 10\n",
        "            count = vacation_filtered[vacation_filtered['Full Name'] == name].shape[0]\n",
        "            print(f\"   ‚Ä¢ {name}: {count} vacation records\")\n",
        "        if len(matched_names) > 10:\n",
        "            print(f\"   ... and {len(matched_names) - 10} more employees\")\n",
        "            \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No filtered users found from Cell 5. Using all vacation data.\")\n",
        "        vacation_filtered = df_vacation_raw.copy()\n",
        "    \n",
        "    # STEP 2: Process vacation data for dashboard use\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üîÑ STEP 2: PROCESSING VACATION DATA FOR DASHBOARD\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Clean and process the vacation data\n",
        "    vacation_processed = vacation_filtered.copy()\n",
        "    \n",
        "    # Convert date columns\n",
        "    date_columns = ['Start date', 'Departure date']\n",
        "    for col in date_columns:\n",
        "        if col in vacation_processed.columns:\n",
        "            vacation_processed[col] = pd.to_datetime(vacation_processed[col], errors='coerce')\n",
        "    \n",
        "    # Focus on actual vacation/leave (not just allocations)\n",
        "    # Filter for records with actual used time or scheduled time\n",
        "    vacation_actual = vacation_processed[\n",
        "        (vacation_processed['Used'] > 0) | (vacation_processed['Scheduled'] > 0)\n",
        "    ].copy()\n",
        "    \n",
        "    print(f\"‚úÖ Processed vacation data: {vacation_actual.shape[0]} records with actual time off\")\n",
        "    \n",
        "    # Categorize vacation types for dashboard\n",
        "    vacation_categories = {\n",
        "        'Vacation': ['Vacation', 'UAE Vacation', 'Work From Anywhere'],\n",
        "        'Sick Leave': ['Sick', 'UAE Sick Time'],\n",
        "        'Parental Leave': ['Parental Leave (UAE)', 'Prenatal Leave'],\n",
        "        'Family Leave': ['Family Caregiver Leave', 'Family Caregiver Leave (UAE)', 'Bereavement'],\n",
        "        'Other': ['Jury Duty', 'UAE Study Leave']\n",
        "    }\n",
        "    \n",
        "    def categorize_vacation_type(vacation_type):\n",
        "        for category, types in vacation_categories.items():\n",
        "            if vacation_type in types:\n",
        "                return category\n",
        "        return 'Other'\n",
        "    \n",
        "    vacation_actual = vacation_actual.copy()\n",
        "    vacation_actual['Vacation_Category'] = vacation_actual['Type'].apply(categorize_vacation_type)\n",
        "    \n",
        "    print(f\"\\nüìä Vacation by category:\")\n",
        "    print(vacation_actual['Vacation_Category'].value_counts())\n",
        "    \n",
        "    # STEP 3: Create vacation summary for dashboard integration\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä STEP 3: CREATING VACATION SUMMARY FOR DASHBOARD\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Create monthly vacation summary\n",
        "    current_date = pd.Timestamp.now()\n",
        "    \n",
        "    # Create date range for next 4 months (current + 3 future)\n",
        "    date_range = pd.date_range(\n",
        "        start=current_date.replace(day=1),\n",
        "        periods=4,\n",
        "        freq='MS'  # Month start\n",
        "    )\n",
        "    \n",
        "    print(f\"üìÖ Dashboard date range: {date_range[0].strftime('%Y-%m')} to {date_range[-1].strftime('%Y-%m')}\")\n",
        "    \n",
        "    # Create vacation summary by person and month\n",
        "    vacation_summary = []\n",
        "    \n",
        "    for _, row in vacation_actual.iterrows():\n",
        "        start_date = row['Start date']\n",
        "        departure_date = row['Departure date']\n",
        "        \n",
        "        # Skip if no valid dates\n",
        "        if pd.isna(start_date):\n",
        "            continue\n",
        "            \n",
        "        # Use departure date if available, otherwise assume single day\n",
        "        end_date = departure_date if pd.notna(departure_date) else start_date\n",
        "        \n",
        "        # Check if vacation overlaps with our dashboard period\n",
        "        for month_start in date_range:\n",
        "            month_end = month_start + pd.offsets.MonthEnd(0)\n",
        "            \n",
        "            # Check if vacation overlaps with this month\n",
        "            if start_date <= month_end and end_date >= month_start:\n",
        "                vacation_summary.append({\n",
        "                    'Full_Name': row['Full Name'],\n",
        "                    'First_Name': row['First Name'],\n",
        "                    'Last_Name': row['Last Name'],\n",
        "                    'Employee_Number': row['Employee Number'],\n",
        "                    'Month': month_start,\n",
        "                    'Vacation_Type': row['Type'],\n",
        "                    'Vacation_Category': row['Vacation_Category'],\n",
        "                    'Days_Used': row['Used'],\n",
        "                    'Days_Scheduled': row['Scheduled'],\n",
        "                    'Start_Date': start_date,\n",
        "                    'End_Date': end_date,\n",
        "                    'Job_Title': row['Job Title'],\n",
        "                    'Office_Location': row['Office Location']\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df_vacation_summary = pd.DataFrame(vacation_summary)\n",
        "    \n",
        "    if not df_vacation_summary.empty:\n",
        "        print(f\"‚úÖ Vacation summary created: {df_vacation_summary.shape[0]} month-person records\")\n",
        "        print(f\"   üìä Covering {df_vacation_summary['Full_Name'].nunique()} unique employees\")\n",
        "        print(f\"   üìÖ Across {df_vacation_summary['Month'].nunique()} months\")\n",
        "        \n",
        "        # Show sample of vacation summary\n",
        "        print(f\"\\nüìä Sample vacation summary:\")\n",
        "        sample_cols = ['Full_Name', 'Month', 'Vacation_Category', 'Days_Used', 'Days_Scheduled']\n",
        "        print(df_vacation_summary[sample_cols].head(10).to_string(index=False))\n",
        "        \n",
        "        # Show monthly vacation totals\n",
        "        print(f\"\\nüìä Monthly vacation totals:\")\n",
        "        monthly_totals = df_vacation_summary.groupby('Month').agg({\n",
        "            'Days_Used': 'sum',\n",
        "            'Days_Scheduled': 'sum',\n",
        "            'Full_Name': 'nunique'\n",
        "        }).round(1)\n",
        "        monthly_totals.columns = ['Total_Days_Used', 'Total_Days_Scheduled', 'Unique_Employees']\n",
        "        print(monthly_totals)\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No vacation data found for the dashboard period\")\n",
        "        df_vacation_summary = pd.DataFrame()\n",
        "    \n",
        "    # STEP 4: Prepare for integration with main dataset\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"üîó STEP 4: PREPARING FOR INTEGRATION WITH MAIN DATASET\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Create a version that can be merged with the main 10k dataset\n",
        "    if not df_vacation_summary.empty and 'df_10k' in locals() and df_10k is not None:\n",
        "        print(\"‚úÖ Attempting to merge vacation data with main 10k dataset...\")\n",
        "        \n",
        "        # Try to match by name\n",
        "        vacation_for_merge = df_vacation_summary.copy()\n",
        "        \n",
        "        # Group by person and month to avoid duplicates\n",
        "        vacation_monthly = vacation_for_merge.groupby(['Full_Name', 'Month']).agg({\n",
        "            'Days_Used': 'sum',\n",
        "            'Days_Scheduled': 'sum',\n",
        "            'Vacation_Category': lambda x: ', '.join(x.unique()),\n",
        "            'First_Name': 'first',\n",
        "            'Last_Name': 'first',\n",
        "            'Employee_Number': 'first'\n",
        "        }).reset_index()\n",
        "        \n",
        "        print(f\"   üìä Monthly vacation data ready for merge: {vacation_monthly.shape[0]} records\")\n",
        "        print(f\"   üë• For {vacation_monthly['Full_Name'].nunique()} unique employees\")\n",
        "        \n",
        "        # Store for use in later cells\n",
        "        df_vacation = vacation_actual  # Full detailed data\n",
        "        df_vacation_monthly = vacation_monthly  # Monthly summary for dashboard\n",
        "        \n",
        "        print(f\"\\n‚úÖ Vacation data processing complete!\")\n",
        "        print(f\"   üìä df_vacation: {df_vacation.shape[0]} detailed records\")\n",
        "        print(f\"   üìä df_vacation_monthly: {df_vacation_monthly.shape[0]} monthly summaries\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Cannot merge with main dataset - df_10k not available\")\n",
        "        df_vacation = vacation_actual\n",
        "        df_vacation_monthly = df_vacation_summary\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ VACATION DATA PROCESSING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Cannot process vacation data - loading failed\")\n",
        "    df_vacation = None\n",
        "    df_vacation_monthly = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Salesforce data loaded successfully: (1787, 19)\n",
            "\n",
            "üìä Column names:\n",
            "['Probability', 'Account Name', 'Engagement Name', 'Created Date', 'Schedule Month', 'Region', 'Schedule Amount', 'Intacct Project ID', 'Project Code', 'Engagement Launch Date', 'Primary Partner', 'Fee', 'Engagement ID', 'Expense Budget', 'Engagement End Date', 'Office', 'Industry', '_BATCH_ID_', '_BATCH_LAST_RUN_']\n",
            "\n",
            "üìä Data types:\n",
            "Probability                object\n",
            "Account Name               object\n",
            "Engagement Name            object\n",
            "Created Date               object\n",
            "Schedule Month             object\n",
            "Region                     object\n",
            "Schedule Amount           float64\n",
            "Intacct Project ID         object\n",
            "Project Code               object\n",
            "Engagement Launch Date     object\n",
            "Primary Partner           float64\n",
            "Fee                       float64\n",
            "Engagement ID              object\n",
            "Expense Budget            float64\n",
            "Engagement End Date        object\n",
            "Office                     object\n",
            "Industry                   object\n",
            "_BATCH_ID_                float64\n",
            "_BATCH_LAST_RUN_           object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# [Cell 7] Load Salesforce Opportunity Data\n",
        "try:\n",
        "    df_salesforce = pd.read_csv('../data/Salesforce Opportunity Data.csv')\n",
        "    print(f\"‚úÖ Salesforce data loaded successfully: {df_salesforce.shape}\")\n",
        "    print(\"\\nüìä Column names:\")\n",
        "    print(df_salesforce.columns.tolist())\n",
        "    print(\"\\nüìä Data types:\")\n",
        "    print(df_salesforce.dtypes)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Salesforce data: {e}\")\n",
        "    df_salesforce = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ US Working Hours data loaded successfully: (145, 17)\n",
            "\n",
            "üìä Column names:\n",
            "['Month', 'Start Date', 'End Date', 'Holiday #1', 'Holiday #2', 'Holiday #3', 'Holiday #4', 'Net Working Hours', 'Column_9', 'First Closure Day', 'Billable Days', 'Total Closures', 'Column_13', 'Column_14', 'Column_15', 'Column_16', 'Column_17']\n",
            "\n",
            "üìÖ Sample data:\n",
            "       Month  Net Working Hours  Billable Days\n",
            "0 2014-01-01                189            NaN\n",
            "1 2014-02-01                180            NaN\n",
            "2 2014-03-01                189            NaN\n",
            "3 2014-04-01                198            NaN\n",
            "4 2014-05-01                189            NaN\n",
            "5 2014-06-01                189            NaN\n",
            "6 2014-07-01                198            NaN\n",
            "7 2014-08-01                189            NaN\n",
            "8 2014-09-01                189            NaN\n",
            "9 2014-10-01                207            NaN\n"
          ]
        }
      ],
      "source": [
        "# [Cell 8] Load US Working Hours Data\n",
        "try:\n",
        "    df_us_hours = pd.read_csv('../data/Working Hours For US.csv')\n",
        "    print(f\"‚úÖ US Working Hours data loaded successfully: {df_us_hours.shape}\")\n",
        "    print(\"\\nüìä Column names:\")\n",
        "    print(df_us_hours.columns.tolist())\n",
        "    # Convert Month column to datetime\n",
        "    df_us_hours['Month'] = pd.to_datetime(df_us_hours['Month'])\n",
        "    print(\"\\nüìÖ Sample data:\")\n",
        "    print(df_us_hours[['Month', 'Net Working Hours', 'Billable Days']].head(10))\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading US working hours data: {e}\")\n",
        "    df_us_hours = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ UAE Working Hours data loaded successfully: (49, 16)\n",
            "\n",
            "üìä Column names:\n",
            "['Month', 'Start Date', 'End Date', 'Holiday #1', 'Holiday #2', 'Holiday #3', 'Holiday #4', 'Holiday #5', 'Net Working Hours', 'Working Days', 'Column_11', 'Column_12', 'First Closure Day', 'Billable Days', 'Total Closures', 'Column_16']\n",
            "\n",
            "üîç First few rows:\n",
            "        Month  Start Date    End Date  Holiday #1  Holiday #2 Holiday #3  \\\n",
            "0  2022-01-01  2022-01-01  2022-01-31  2022-01-21  2022-01-24        NaN   \n",
            "1  2022-02-01  2022-02-01  2022-02-28  2022-02-21  2022-02-18        NaN   \n",
            "2  2022-03-01  2022-03-01  2022-03-31  2022-03-25         NaN        NaN   \n",
            "3  2022-04-01  2022-04-01  2022-04-30  2022-04-29         NaN        NaN   \n",
            "4  2022-05-01  2022-05-01  2022-05-31  2022-05-27  2022-05-30        NaN   \n",
            "\n",
            "  Holiday #4 Holiday #5  Net Working Hours  Working Days Column_11 Column_12  \\\n",
            "0        NaN        NaN                171           NaN       NaN       NaN   \n",
            "1        NaN        NaN                162           NaN       NaN       NaN   \n",
            "2        NaN        NaN                198           NaN       NaN       NaN   \n",
            "3        NaN        NaN                180           NaN       NaN       NaN   \n",
            "4        NaN        NaN                180           NaN       NaN       NaN   \n",
            "\n",
            "  First Closure Day  Billable Days  Total Closures  Column_16  \n",
            "0               NaN            NaN             NaN        NaN  \n",
            "1               NaN            NaN             NaN        NaN  \n",
            "2               NaN            NaN             NaN        NaN  \n",
            "3               NaN            NaN             NaN        NaN  \n",
            "4               NaN            NaN             NaN        NaN  \n"
          ]
        }
      ],
      "source": [
        "# [Cell 9] Load UAE Working Hours Data\n",
        "try:\n",
        "    df_uae_hours = pd.read_csv('../data/UAE Working Hours.csv')\n",
        "    print(f\"‚úÖ UAE Working Hours data loaded successfully: {df_uae_hours.shape}\")\n",
        "    print(\"\\nüìä Column names:\")\n",
        "    print(df_uae_hours.columns.tolist())\n",
        "    print(\"\\nüîç First few rows:\")\n",
        "    print(df_uae_hours.head())\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading UAE working hours data: {e}\")\n",
        "    df_uae_hours = None"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
